{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2e-s9vjTryy"
   },
   "outputs": [],
   "source": [
    "#fetching the data from MIT-BIH \n",
    "!wget -r -N -c -np https://physionet.org/files/nstdb/1.0.0/\n",
    "!wget -r -N -c -np https://physionet.org/files/mitdb/1.0.0/\n",
    "!pip install wfdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62IgusaFTtJs"
   },
   "outputs": [],
   "source": [
    "#imports here\n",
    "import wfdb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler, normalize\n",
    "from statistics import variance as var\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwokPlAyTwD6"
   },
   "outputs": [],
   "source": [
    "#reading the muscle artifact noise and spliting it into 650 parts each of length 1000 and normalizing it\n",
    "#ma contains 650000 sample points\n",
    "ma,f = wfdb.rdsamp('/content/physionet.org/files/nstdb/1.0.0/ma', sampto=650000)\n",
    "ma=np.array(ma)\n",
    "noise=[]\n",
    "v=0\n",
    "for i in range(649):\n",
    "  y=[]\n",
    "  for k in range(1000):\n",
    "    y.append(ma[v+k][0])\n",
    "  z=[]\n",
    "  p=max(y)\n",
    "  q=min(y)  \n",
    "  for l in range(len(y)):\n",
    "      t=(y[l]-q)/(p-q)\n",
    "      z.append(t) \n",
    "  noise.append(z)\n",
    "  v=v+1000\n",
    "plt.plot(noise[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dlZAO0gLTyrp"
   },
   "outputs": [],
   "source": [
    "#reading data spliting and normalizing it........\n",
    "data=[]\n",
    "rec=['100','101','102','103','104','105','106','107','108','109','111','112','113','114','115','116','117','118','119','121','122','123','124','200','201','202','203','205','207','208','209','210','212','213','214','215','217','219','220','221','222','223','228','230','231','232','233','234']\n",
    "\n",
    "for string in rec:\n",
    "  record= wfdb.rdsamp('/content/physionet.org/files/mitdb/1.0.0/'+string, sampto=650000)\n",
    "  ann = wfdb.rdann('/content/physionet.org/files/mitdb/1.0.0/'+string, 'atr', sampto=650000)\n",
    "  I = record[0][:, 0]\n",
    "  a=0\n",
    "  e=1000\n",
    "  for i in range(650):\n",
    "    z=[]\n",
    "    d=I[a:e]\n",
    "    p=max(d)\n",
    "    q=min(d)\n",
    "    for l in range(len(d)):\n",
    "      t=(d[l]-q)/(p-q)\n",
    "      z.append(t)\n",
    "    data.append(z)\n",
    "    a+=1000\n",
    "    e+=1000\n",
    "#plotting the acquired ecg signal, mind that we are only taking lead I\n",
    "plt.plot(data[1])\n",
    "plt.xlabel('samples')\n",
    "plt.ylabel('mV')\n",
    "#dividing data in train and test test samples\n",
    "train_data=[]\n",
    "test_data=[]\n",
    "for i in range(int(len(data)*0.8)):\n",
    "  train_data.append(data[i])\n",
    "for j in range(int(len(data)*0.8),len(data)):\n",
    "  test_data.append(data[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rUN_k5A7T1P0"
   },
   "outputs": [],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "\n",
    "# Prints: [8.0, 6.0]\n",
    "print(\"Current size:\", fig_size)\n",
    "# Set figure width to 12 and height to 9\n",
    "fig_size[0] = 10\n",
    "fig_size[1] = 2\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.plot(data[1000])\n",
    "plt.xlabel('samples')\n",
    "plt.ylabel('mV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0cq_KN-T3Y9"
   },
   "outputs": [],
   "source": [
    "#noise_ma_data base\n",
    "import random\n",
    "n=[]\n",
    "n_tr=[]\n",
    "n_ts=[]\n",
    "noise_ma_data=[]\n",
    "for i in range(31200):\n",
    "  l=random.randrange(0,200)\n",
    "  n.append(noise[l])\n",
    "  p=list(map(add,data[i],noise[l])) \n",
    "  p=np.array(p)\n",
    "  noise_ma_data.append(p)\n",
    "trainma_data=[]\n",
    "testma_data=[]\n",
    "for i in range(int(len(noise_ma_data)*0.8)):\n",
    "  trainma_data.append(noise_ma_data[i])\n",
    "  n_tr.append(n[i])\n",
    "for j in range(int(len(noise_ma_data)*0.8),len(noise_ma_data)):\n",
    "  testma_data.append(noise_ma_data[j])\n",
    "  n_ts.append(n[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_NCMR0AT5uk"
   },
   "outputs": [],
   "source": [
    "plt.plot(noise_ma_data[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJd5UrsqT-14"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Metrics for future implementation\n",
    "#to calulate snr, rmse, max_abs_val, normalized_cross_correlation\n",
    "#https://github.com/scipy/scipy/issues/9097\n",
    "def urms(x):\n",
    "  y=[]\n",
    "  for i in range(len(x)):\n",
    "    y.append(x[i]**2)\n",
    "  s=sum(y)\n",
    "  return s\n",
    "def signaltonoise(a,b):\n",
    "  a=urms(a)\n",
    "  b=urms(b)\n",
    "  snr=a/b\n",
    "  return np.log(snr)*10\n",
    "#intellipaat.com/community/1269/is-there-a-library-function-for-root-mean-square-error-rmse-in-python\n",
    "def rmse(predictions, targets): \n",
    "    return np.sqrt(((np.array(predictions) - np.array(targets)) ** 2).mean())\n",
    "#https://stackoverflow.com/questions/44864633/pythonic-way-to-find-maximum-absolute-value-of-list\n",
    "def max_abs_val(samp_list):\n",
    "  max_abs_value = samp_list[0]\n",
    "  for num in samp_list:\n",
    "      if abs(num) > max_abs_value:\n",
    "          max_abs_value = abs(num)    \n",
    "  return max_abs_value\n",
    "#www.researchgate.net/post/How_can_one_calculate_normalized_cross_correlation_between_two_arrays\n",
    "def normalized_cross_corr(x,y):\n",
    "  x,y=np.array(x),np.array(y)\n",
    "  N=len(x)\n",
    "  a=(np.sqrt(var(x)*var(y)))\n",
    "  b=(x - x.mean())* (y - y.mean())\n",
    "  ncc = (1/N)*(sum(b))\n",
    "  return ncc/a\n",
    "def convert_numpy_tensor(arr):\n",
    "  arr=np.array(arr)\n",
    "  ten=torch.from_numpy(arr)\n",
    "  return ten    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_VDtD2VT_kG"
   },
   "outputs": [],
   "source": [
    "#AutoEncoder \n",
    "# define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(1000,500)\n",
    "        self.fc2 = nn.Linear(500,250)\n",
    "        self.fc3 = nn.Linear(250,100)\n",
    "        self.fc4=nn.Linear(100,250)\n",
    "        self.fc5=nn.Linear(250,500)\n",
    "        self.fc6=nn.Linear(500,1000)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    def encoder(self, x):\n",
    "        x=x.squeeze()\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x=F.relu(x)\n",
    "        return x\n",
    "    def decoder(self,x):\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        x=x.view(-1)\n",
    "        return x\n",
    "# initialize the NN\n",
    "model = Net()\n",
    "model.cuda()\n",
    "print(model)\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z2MK5A30UBRE"
   },
   "outputs": [],
   "source": [
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.09,momentum=0.2)\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    model.train() # prep model for training\n",
    "    for i in range(int(len(trainma_data))):\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        b=convert_numpy_tensor(trainma_data[i])\n",
    "        b=b.cuda()\n",
    "        compressed = model.encoder(b)\n",
    "        output=model.decoder(compressed)\n",
    "        g=convert_numpy_tensor(train_data[i])\n",
    "        g=g.cuda()\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,g.squeeze())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*b.size(0)\n",
    "    model.eval() # prep model for evaluation\n",
    "    for j in range(int(len(testma_data))):\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        c=torch.from_numpy(testma_data[j])\n",
    "        c=c.cuda()\n",
    "        compressed = model.encoder(c)\n",
    "        output=model.decoder(compressed)\n",
    "        h=convert_numpy_tensor(test_data[j])\n",
    "        h=h.cuda()\n",
    "        # calculate the loss\n",
    "        loss = criterion(output,h.squeeze())\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*c.size(0)\n",
    "        if (j==len(test_data)-1) and (epoch==n_epochs):\n",
    "            plt.plot(testma_data[j])\n",
    "            plt.plot(output.detach().numpy())\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/int(len(trainma_data)*0.3)\n",
    "    valid_loss = valid_loss/int(len(testma_data)*0.3)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XtQ3nkkgUD7Q"
   },
   "outputs": [],
   "source": [
    "#check for viz\n",
    "c=torch.from_numpy(noise_ma_data[28000])\n",
    "c=c.cuda()\n",
    "compressed = model.encoder(c)\n",
    "output=model.decoder(compressed)\n",
    "output=output.detach().cpu().numpy()\n",
    "plt.plot(output)\n",
    "plt.plot(noise_ma_data[28000])\n",
    "plt.plot(data[28000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EitzDULOUF6P"
   },
   "outputs": [],
   "source": [
    "#testing\n",
    "snr_original=0\n",
    "snr_noise_added=0\n",
    "snr_denoised=0\n",
    "ncc=0\n",
    "rootms=0\n",
    "for i in range(20000,30000):\n",
    "  c=torch.from_numpy(noise_ma_data[i])\n",
    "  c=c.cuda()\n",
    "  compressed = model.encoder(c)\n",
    "  output=model.decoder(compressed)\n",
    "  output=output.detach().cpu().numpy()\n",
    "  d=data[i]\n",
    "  snr_original=snr_original+signaltonoise(d,n[i])\n",
    "  snr_noise_added=snr_noise_added+signaltonoise(noise_ma_data[i],n[i])\n",
    "  snr_denoised=snr_denoised+signaltonoise(output,output-d)\n",
    "  ncc=ncc+normalized_cross_corr(d,output)\n",
    "  rootms=rootms+rmse(d,output)\n",
    "snr_original=snr_original/10000\n",
    "snr_noise_added=snr_noise_added/10000\n",
    "snr_denoised=snr_denoised/10000\n",
    "rootms=rootms/10000\n",
    "ncc=ncc/10000\n",
    "print('snr of original signal {} db'.format(snr_original))\n",
    "print('snr of noise_added signal {} db'.format(snr_noise_added))\n",
    "print('snr of denoised signal {} db'.format(snr_denoised))\n",
    "print('ncc of denoised signal {}'.format(ncc))\n",
    "print('root mean square error {}'.format(rootms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImZj6XXPUH5R"
   },
   "outputs": [],
   "source": [
    "#store the saved model\n",
    "torch.save(model.state_dict(), 'checkpointma_10X.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ddLd-zlUKoZ"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('checkpointma_10X.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "py-cJHiIUO9C"
   },
   "outputs": [],
   "source": [
    "#loading the model..........\n",
    "model.load_state_dict(torch.load('checkpoint_10X_2.pt'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MuscleAritifact",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
